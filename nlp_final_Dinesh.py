# [Graduate-Level]  -*- coding: utf-8 -*-
"""NLP_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uGb-QZaXkKodsyl2RCCdolqJFI0jene0
"""

#Commented out IPython magic to ensure Python compatibility.
#Initialize libraries for data processing, visualization, and machine learning.
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
#%matplotlib inline
import seaborn as sns

import shutil
import os
import kagglehub
from google.colab import drive

from sklearn.metrics import confusion_matrix,classification_report
from collections import Counter
from wordcloud import WordCloud

from sklearn.preprocessing import LabelEncoder

#classifier
from sklearn.model_selection import train_test_split

from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import classification_report, accuracy_score

#Mount Google Drive
drive.mount('/content/drive')

#Download datasets to default location
snehaanbhawal_resume_dataset_path = kagglehub.dataset_download('snehaanbhawal/resume-dataset')
pranavvenugo_resume_and_job_description_path = kagglehub.dataset_download('pranavvenugo/resume-and-job-description')

#Define target Google Drive paths
google_drive_path = '/content/drive/MyDrive/Datasets'
#os.makedirs(google_drive_path, exist_ok=True)


#Move downloaded files to Google Drive
shutil.move(snehaanbhawal_resume_dataset_path, os.path.join(google_drive_path, 'resume-dataset'))
shutil.move(pranavvenugo_resume_and_job_description_path, os.path.join(google_drive_path, 'job-description'))

print(f"Resume dataset moved to: {google_drive_path}/resume-dataset")
print(f"Job description dataset moved to: {google_drive_path}/job-description")


import numpy as np 
import pandas as pd 

#Input data files are available in the read-only "../input/" directory

import os
for dirname, _, filenames in os.walk(google_drive_path):
    for filename in filenames:
        print(os.path.join(dirname, filename))

#You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
#You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

resume_data = pd.read_csv("/content/drive/MyDrive/Datasets/resume-dataset/Resume/Resume.csv")
resume_data.head()

resume_data.info()

#Check how much data is available for our training model.
resume_data.shape

resume_data['Category'].unique()

#check how many resume are available in every category.
resume_data['Category'].value_counts()

len(resume_data['Category'].value_counts())

"""#Exploratory Data Analysis"""

#Visualize the distribution of categories
plt.figure(figsize=(15, 6))
sns.countplot(x=resume_data['Category'])
plt.xticks(rotation=90)
plt.title('Category Distribution')
plt.show()

#create pie plot

counts=resume_data['Category'].value_counts()
labels=resume_data['Category'].unique()

plt.figure(figsize=(15,12))
plt.pie(counts,labels= labels, autopct='%1.1f%%',shadow=True, colors=plt.cm.plasma(np.linspace(0,1,3)))
plt.title('Distribution of Categories')
plt.axis('equal')
plt.show()

#Create a color palette
colors = plt.cm.rainbow(np.linspace(0, 1, len(labels)))

#Create pie plot
plt.figure(figsize=(12, 8))
plt.pie(counts, labels=labels, autopct='%1.1f%%', shadow=True, colors=colors, startangle=180)

#Add a circle at the center to simulate depth perception
centre_circle = plt.Circle((0, 0), 0.70, fc='white')
fig = plt.gcf()
fig.gca().add_artist(centre_circle)

#Equal aspect ratio ensures that pie is drawn as a circle.
plt.axis('equal')

plt.title("Resume Categories", fontsize=16)
plt.show()

import plotly.graph_objects as go

#Count the occurrences of each category
counts=resume_data['Category'].value_counts()
labels=resume_data['Category'].unique()

#Create a pie chart using Plotly
fig = go.Figure(data=[go.Pie(labels=labels, values=counts)])

#Set layout options
fig.update_layout(title='Resume Categories')

#Show the pie chart
fig.show()

#Count the occurrences of each category
category_counts = resume_data['Category'].value_counts()

#Create a DataFrame with unique category names and their counts
category_counts_df = category_counts.reset_index()
category_counts_df.columns = ['Category', 'Count']

#Create a violin plot
plt.figure(figsize=(15, 10))
sns.scatterplot(data=category_counts_df, x="Category", y="Count" ,size="Count",hue="Count", sizes=(20, 200), hue_norm=(0, 7), palette="deep",legend="full")
plt.xlabel('Category')
plt.ylabel('Count')
plt.title('Distribution of Category Counts using Scatterplot')
plt.xticks(rotation=90)
plt.show()

#Combine text from Category columns
combined_text = ' '.join(resume_data['Category'])

#Tokenize and count word frequency
word_counts = Counter(combined_text.split())

#Create a word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_counts)
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud - Most Frequent Words in Category')
plt.show()

"""#Handling Missing Values and Text Cleaning"""

#Check for missing values
missing_values = resume_data.isnull().sum()

#Visualize missing values
plt.figure(figsize=(8, 5))
sns.heatmap(resume_data.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Values Heatmap')
plt.show()

resume_data.isnull().sum()

def extract_text_from_pdf(file_path):
    reader = PdfReader(file_path)
    text = "".join(page.extract_text() for page in reader.pages)
    return text

from nltk import pos_tag, sent_tokenize, word_tokenize
from nltk.corpus import stopwords
import string
import re
def preprocess_text(text):
    text = text.lower()
    text = re.sub('[^a-zA-Z]', ' ', text)
    sentences = sent_tokenize(text)
    features = {'feature': ""}
    stop_words = set(stopwords.words("english"))
    for sent in sentences:
        if any(criteria in sent for criteria in ['skills', 'education']):
            words = word_tokenize(sent)
            words = [word for word in words if word not in stop_words]
            tagged_words = pos_tag(words)
            filtered_words = [word for word, tag in tagged_words if tag not in ['DT', 'IN', 'TO', 'PRP', 'WP']]
            features['feature'] += " ".join(filtered_words)
    return features

def process_resume_data(df):
    id = df['ID']
    category = df['Category']
    text = extract_text_from_pdf(f"/content/drive/MyDrive/Datasets/resume-dataset/data/data/{category}/{id}.pdf")
    features = preprocess_text(text)
    df['Feature'] = features['feature']
    return df
'''
!pip install transformers
!pip install PyPDF2
!pip install PyPDF
from pypdf import PdfReader

#Remove nltk data directory (optional to start fresh)
!rm -rf /root/nltk_data

#Re-download 'punkt' package
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger_eng')

import nltk
nltk.data.path.append('/root/nltk_data')
from nltk.tokenize import word_tokenize
'''
num_desc = 15
resume_data = pd.read_csv("/content/drive/MyDrive/Datasets/resume-dataset/Resume/Resume.csv")
resume_data = resume_data.drop(["Resume_html"], axis=1)
resume_data = resume_data.apply(process_resume_data, axis=1)
resume_data = resume_data.drop(columns=['Resume_str'])
resume_data.to_csv("/content/drive/MyDrive/Datasets/trained/resumedata.csv", index=False)

job_description = pd.read_csv("/content/drive/MyDrive/Datasets/job-description/training_data.csv")
job_description = job_description[["job_description", "position_title"]][:num_desc]
job_description['Features'] = job_description['job_description'].apply(lambda x : preprocess_text(x)['feature'])

resume_data

categories = np.sort(resume_data['Category'].unique())
categories
#create new df for corpus and category
df_categories = [resume_data[resume_data['Category'] == category].loc[:, ['Feature', 'Category']] for category in categories]

from wordcloud import WordCloud
def wordcloud(df):
    txt = ' '.join(txt for txt in resume_data['Feature'])
    wordcloud = WordCloud(
        height=2000,
        width=4000
    ).generate(txt)

    return wordcloud

import matplotlib.pyplot as plt
plt.figure(figsize=(32, 20))

for i, category in enumerate(categories):
    wc = wordcloud(df_categories[i])

    plt.subplot(5, 5, i + 1).set_title(category)
    plt.imshow(wc)
    plt.axis('off')
    plt.plot()

plt.show()
plt.close()

def remove_extra_word(text):

    extra_word=['company', 'name', 'citi', 'state', 'work', 'manag', 'project'] # extra words
    words = text.split()  # Split the text into words

    #Filter out the extra words
    filter_word = [word for word in words if word not in extra_word]

    filter_text = ' '.join(filter_word)

    return filter_text


#apply resume_data['Cleaned_Resume']

resume_data['Feature']=resume_data['Feature'].apply(lambda x:remove_extra_word(x))

plt.figure(figsize=(32, 20))

for i, category in enumerate(categories):
    wc = wordcloud(df_categories[i])

    plt.subplot(5, 5, i + 1).set_title(category)
    plt.imshow(wc)
    plt.axis('off')
    plt.plot()

plt.show()
plt.close()

plt.figure(figsize=(32, 20))

for i, category in enumerate(categories):
    wc = wordcloud(df_categories[i])

    plt.subplot(5, 5, i + 1).set_title(category)
    plt.imshow(wc)
    plt.axis('off')
    plt.plot()

plt.show()
plt.close()

#Split data into training and validation
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(resume_data['Feature'], resume_data['Category'], test_size=0.15, random_state=42, stratify=resume_data['Category'])


#Print the sizes of the split datasets
print("Train data size:", X_train.shape)
print("Validation data size:", X_test.shape)

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf=TfidfVectorizer(stop_words='english',max_features=800)
tfidf_train_vectors = tfidf.fit_transform(X_train)
tfidf_test_vectors =tfidf.transform(X_test)

tfidf_jobDesc_vectors = tfidf.fit_transform(job_desc['Features'])
tfidf_train_vectors

tfidf_jobDesc_vectors_dense = tfidf_jobDesc_vectors.toarray()
tfidf_train_vectors_dense = tfidf_train_vectors.toarray()
tfidf_train_vectors_dense.shape, tfidf_jobDesc_vectors_dense.shape

tfidf_test_vectors

tfidf.get_feature_names_out()

from sklearn.metrics.pairwise import cosine_similarity
k = 5
result_df = pd.DataFrame(columns=['jobId', 'resumeId', 'similarity', 'domainResume', 'domainDesc'])
for i, job_desc_emb in enumerate(tfidf_jobDesc_vectors_dense):
    job_desc_id = i
    job_title = job_description['position_title'].iloc[i]

    #Compute cosine similarities between the current job description and all resumes
    similarities = cosine_similarity([job_desc_emb], tfidf_train_vectors )
    top_k_indices = np.argsort(similarities[0])[::-1][:k]

    #Extract the relevant information and add it to the result DataFrame
    for j in top_k_indices:
        resume_id = resume_data['ID'].iloc[j]
        work_domain = resume_data['Category'].iloc[j]
        similarity_score = similarities[0][j]

        result_df.loc[i+j] = [job_desc_id, resume_id, similarity_score, work_domain,job_title ]


#Sort the results by similarity score (descending)
result_df = result_df.sort_values(by='similarity', ascending=False)

result_df.head()

result_group=result_df.groupby("jobId")
result_group

num_desc = 15
for i in range(num_desc):
    print()
    print("jobId---cosineSimilarity---domainResume---domainDesc")
    print(result_group.get_group(i).values[0])
    print()

"""#Feedback Genration"""

#Install necessary libraries (if not installed)

#Import libraries
import os
from keybert import KeyBERT
import spacy
import pandas as pd
from textblob import TextBlob
from transformers import pipeline, AutoTokenizer
from sentence_transformers import SentenceTransformer
import logging

#Logging setup
logging.basicConfig(level=logging.INFO)

#Initialize spaCy, KeyBERT, and Sentiment Analysis Pipeline
nlp = spacy.load("en_core_web_sm")
kw_model = KeyBERT(model=SentenceTransformer('all-MiniLM-L6-v2', device='cuda'))  # Use CUDA
sentiment_pipeline = pipeline("sentiment-analysis", device=0)  # Use GPU
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

#Load datasets
resume_data_path = "/content/drive/MyDrive/Datasets/trained/resumedata_project3.csv"
job_description_data_path = "/content/drive/MyDrive/Datasets/job-description/training_data.csv"
output_feedback_path = "/content/drive/MyDrive/Datasets/feedback/recommendation_feedback_full.csv"

resume_data = pd.read_csv(resume_data_path)  # Load the entire resume dataset
job_description_data = pd.read_csv(job_description_data_path)[["job_description", "position_title"]]  # Load all job descriptions

#Pre-extract job keywords
job_descriptions = job_description_data['job_description'].tolist()
job_titles = job_description_data['position_title'].tolist()

def truncate_text(text, max_length=512):
    tokens = tokenizer.tokenize(text)
    truncated_tokens = tokens[:max_length - 2]  # Reserve space for special tokens
    return tokenizer.convert_tokens_to_string(truncated_tokens)

def extract_keywords(text):
    try:
        keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=5)
        return [kw[0] for kw in keywords]
    except Exception as e:
        logging.error(f"Keyword extraction failed for text: {text[:50]}... Error: {e}")
        return []

#Extract job keywords
job_keywords = [set(extract_keywords(job_text)) for job_text in job_descriptions]

def categorize_feedback(feedback):
    categories = {
        "Technical Skills": [
            "Python", "AWS", "Google Cloud", "SQL", "Docker",
            "Engineering", "Information Technology", "Digital Media",
            "Consultant", "Automobile", "Construction", "Machine Learning",
            "Artificial Intelligence", "Data Science", "Blockchain",
            "Web Development", "Cloud Computing", "DevOps", "Cybersecurity",
            "Networking", "Linux", "Big Data", "Kubernetes", "Programming",
            "Software Development", "Database Management", "Automation"
        ],
        "Soft Skills": [
            "team management", "communication", "leadership",
            "Public Relations", "Sales", "Fitness", "Advocate",
            "problem solving", "time management", "creativity",
            "decision making", "adaptability", "critical thinking",
            "negotiation", "conflict resolution", "interpersonal skills",
            "emotional intelligence", "collaboration", "mentorship",
            "customer service", "presentation skills", "active listening"
        ],
        "Domain Knowledge": [
            "HR", "Finance", "Marketing", "Operations",
            "Banking", "Healthcare", "Aviation", "Arts",
            "Apparel", "Teacher", "Business Development", "Chef",
            "Agriculture", "Designer", "Accountant", "Real Estate",
            "Retail", "Logistics", "Hospitality", "Event Management",
            "Law", "Education", "Media and Journalism", "Tourism",
            "Insurance", "Pharmaceuticals", "Government Services"
        ]
    }
    categorized = {cat: [kw for kw in categories[cat] if kw.lower() in feedback.lower()] for cat in categories}
    return {k: v for k, v in categorized.items() if v}

def analyze_clarity(text):
    filler_words = ["um", "like", "you know", "basically", "actually"]
    text_blob = TextBlob(text)
    meaningful_words = [word for word in text.split() if word.lower() not in nlp.Defaults.stop_words]
    return {
        "word_count": len(meaningful_words),
        "sentence_count": len(text_blob.sentences),
        "filler_words": sum(text.lower().count(word) for word in filler_words)
    }

def analyze_tone(text):
    truncated_text = truncate_text(text)
    if len(truncated_text.split()) < 3:
        return {"label": "UNKNOWN", "score": 0.0}
    try:
        result = sentiment_pipeline(truncated_text)
        return result[0]  
    except Exception as e:
        logging.error(f"Sentiment analysis failed for text: {truncated_text[:50]}... Error: {e}")
        return {"label": "UNKNOWN", "score": 0.0}

def generate_feedback(resume_keywords, job_keywords, job_title):
    """Generate feedback based on presence or absence of skills."""
    feedback_list = []

    for kw in job_keywords:
        if kw not in resume_keywords:
            #Constructive Feedback for missing keywords
            feedback_list.append(f"Add '{kw}' under the 'Skills' or 'Experience' section.")
        else:
            # [Graduate-Level]  Encouraging Feedback for matching keywords
            feedback_list.append(f"Consider highlighting your experience with '{kw}' in the 'Skills' section to enhance alignment with the role.")

    return "\n".join(feedback_list)

def process_resume(row):
    """Generates feedback for a single resume."""
    resume_text = row['Cleaned_Resume']
    resume_id = row['ID']

    if not isinstance(resume_text, str) or not resume_text.strip():
        logging.warning(f"Resume ID {resume_id} has no valid text. Skipping.")
        return [{"Resume_ID": resume_id, "Job_Title": None, "Feedback": "No valid resume text provided."}]

    resume_keywords = set(extract_keywords(resume_text))
    clarity = analyze_clarity(resume_text)

    feedback_list = []
    for job_kw_set, job_title in zip(job_keywords, job_titles):
        #Generate feedback
        feedback = generate_feedback(resume_keywords, job_kw_set, job_title)

        #Compile feedback data
        feedback_list.append({
            "Resume_ID": resume_id,
            "Job_Title": job_title,
            "Feedback": feedback,
            "Clarity": clarity
        })

    return feedback_list

if __name__ == "__main__":
    logging.info("Starting feedback generation...")
    results = [process_resume(row) for _, row in resume_data.iterrows()]

    #Flatten results
    feedback_list = [item for sublist in results for item in sublist]

    #Create a DataFrame
    feedback_df = pd.DataFrame(feedback_list)

    #Add Categorized Feedback
    feedback_df["Categorized_Feedback"] = feedback_df["Feedback"].apply(
        lambda feedback: categorize_feedback(feedback) if isinstance(feedback, str) else {}
    )

    #Save to CSV
    feedback_df.to_csv(output_feedback_path, index=False)
    logging.info(f"Feedback generation completed and saved to {output_feedback_path}")